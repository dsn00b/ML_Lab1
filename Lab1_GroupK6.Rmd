---
title: "Machine Learning: Block 1 Lab 1 (Group K6)"
author: "Yiran Wang, Tore Andersson and Shashi Nagarajan Iyer"
date: "11/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(kknn)
library(ggplot2)
library(data.table)
library(glmnet)
```

## Statement of Contribution:
All assignments were completed independently by each team member and approaches/results were discussed within the group. Reports for assignments 1, 2 and 3 were produced primarily by Yiran, Shashi and Tore respectively.

## 1. Assignment 1 - Handwritten digit recognition with K-NN

### 1.1 Import data and divide it into training, validation and test sets

The first task is to import digit data and divide it into three sets for training, testing and validation respectively with 50%, 25% and 25% of the data. The code is as below:

```{r}
digits <- read.csv("optdigits.csv", header = F)
colnames(digits)[ncol(digits)] <- "number"
digits$number <- as.factor(digits$number)
n <- dim(digits)[1] 
set.seed(12345)
id <- sample(1:n, floor(n*0.5)) 
train <- digits[id,]
id1 <- setdiff(1:n, id) 
set.seed(12345) 
id2 <- sample(id1, floor(n*0.25)) 
validation <- digits[id2,]
id3 <- setdiff(id1,id2) 
test <- digits[id3,]
```


### 1.2 Train KNN model and estimate the result

This task consists of training a knn model with 30 nearest neighbors and estimate the result with confusion metrics and misclassification errors for both the training and test data. This should be done, using the kknn function, with the following code:

```{r, warning=FALSE}
knn_train <- kknn(number ~., train, train, k = 30, kernel = "rectangular")
knn_test <- kknn(number ~., train, test, k = 30, kernel = "rectangular")
pred_train<- fitted(knn_train)
confusion_train <- table(train$number, pred_train)
pred_test <- fitted(knn_test)
confusion_test <- table(test$number, pred_test)
```

The confusion matrix for train data is:

```{r, warning=FALSE}
print(confusion_train)
```

The confusion matrix for test data is:

```{r, warning=FALSE}
print(confusion_test)
```

With these two matrix, we can simply have the accuracy rate based on correct numbers over total numbers for each digits:

```{r, warning=FALSE}
print(diag(confusion_train)/rowSums(confusion_train))
print(diag(confusion_test)/rowSums(confusion_test))
```

For train data, the quality of predicting different digits is with a large range between 91% and 100%. The prediction for digit 0 is 100%, and following with 6, 2, 3 and 7 with 97% to 99% rate. While the lower ones are for digits 4, 9 and 1 with about 91% correct rate. This could indicate that even for train data itself, the model does not fit quite well.

For test data, the accuracy rates for prediction on digits do not have a huge difference than for train data. Some of them are higher than the train data prediction rate such as for 1, 6, 7 and 9. The highest one is 100% for digit 6. And other ones are below the train ones with a lowest rate for digit 4 being 86%. As we can see here, the rates for digits comparing with them for train data do not have an obvious decreasing trending, which means the model generalizes well.

We can also have the mis-classification rate calculated as below:

```{r, warning=FALSE}
missclass <- function(X,X1) { 
  n = length(X)
  return(1-sum(diag(table(X,X1)))/n)
}
mismatch_rate_train <- missclass(train$number, pred_train)
mismatch_rate_test <- missclass(test$number, pred_test)
cat("The mismatch rate for train data is:", mismatch_rate_train, 
    "\nThe mismatch rate for test data is:", mismatch_rate_test)
```

For the overall mismatch rate 0.045 for train data and 0.053 for test data, we can see that the rates for both of them are not low enough and the difference between them is not high, this can mean that the model is a little under fitting.


### 1.3 Identify and plot cases with low and high correct rates

This task is to find cases that are hard to classify and easy to classify and plot them with heatmap plots. This can be done with code as follows:

```{r}
train_eight <- train[which(train$number==8),]
train_eight$prob <- knn_train$prob[which(train$number==8), "8"]
sortedResult <- sort(train_eight[, "prob"], index.return = T)
#sortedResult$x[1:3] #0.1000000 0.1333333 0.1666667
hard_ids <- sortedResult$ix[1:3] #50  43 136
#tail(sortedResult$x, n=c(2)) #1 1
easy_ids <- tail(sortedResult$ix, n=c(2)) #179 183
plotHeatmap <- function(index){
  heatmap(matrix(as.numeric(train_eight[index, 1:64]), 8, 8, byrow = T), Colv = NA, Rowv = NA)
}
# for (i in c(hard_ids, easy_ids)) {
#   plotHeatmap(i)
# }
```
![](Lab1_assignment1_3.jpeg) 

The cases that are hard to see are chosen with lowest correct rates being 0.1000000, 0.1333333 and 0.1666667. While for the easy to identify ones, the rates are both 100%. As the heatmap shows, the cases that has lowest accuracy rates(the top three showing on the image) are hard to recognized as digits 8 when visualizing them as well. All of them are either too blur or the shape is not clear enough to identify as digit 8. While the 100% accurate cases(the bottom two on the image) are very easy to see as 8.


### 1.4 Use different knn models to fit the train data and analyze with miss-classification rate

This is about fitting the train data with K=1..30 knn models and plot the mis-classification rates for train and validation data. Then observe the plot in different perspective. The code shows as following:

```{r, warning=FALSE}
e_t <- 0
e_v <- 0
for (k in 1:30) {
  kt <- kknn(number ~., train, train, k = k, kernel = "rectangular")
  e_t[k] <- missclass(train$number, fitted(kt))
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  e_v[k] <- missclass(validation$number, fitted(kv))
}
plot(1:30, e_v, ylim = c(0,0.06), type = "l", col = "red", xlab = "K", ylab = "miss-classification error rate")
lines(1:30, e_t, ylim = c(0,0.06), col = "blue")
legend(2, 0.06, legend=c("validation", "train"),
       col=c("red", "blue"), lty=1:1, cex=0.8)
```

As k increases, the complexity of the model for both data sets also increases overall. As we can see in the plot, the error rate for the validation data decreases first to 0.025 when K is 3 and then goes up quickly as K increases with a final slight drop till 0.05. While the error rate for the train data starts with 0 when k is 0, and then goes up gradually as K goes up.

The optimal K is a little difficult to detect right away since the differences for different K values are very small, especially when K is about 3 to 7. But I would say maybe 3 is the optimal one based on only the rate we plot here. As at that point, the value for validation rate is the lowest and for train data is also relatively low.

From the perspective of bias-variance trade-off, low bias is not always perfect as it can lead to a higher variance(when error of train data is 0, the error of validation data is not the lowest). When bias is a little bit higher, the variance can be a bit lower as it can generalize new data better. But after certain point, when bias gets too high, the error rate for both train and validation data will be high. In this case, it would be not a suitable model as it may be under-fitting.

With the optimal K value selected based on above analysis, the model for test data can be shown with following code:

```{r, warning=FALSE}
k_test_optik <- kknn(number ~., train, test, k = 3, kernel = "rectangular")
cat("Test mis-classification rate when k=3 is:", missclass(test$number, fitted(k_test_optik)))
cat("Train mis-classification rate when k=3 is:", e_t[3])
cat("Validation mis-classification rate when k=3 is:", e_v[3])
```

The test error being 2.40% with K=3 is about the same as the error rate of validation data and a little higher (about 1.25%) higher than train data. I would say the quality of the model with K=3 is acceptable because with a similar relatively high fitting rate to both validation and test data(not overfitting) and well fitted for train data(not underfitting).


### 1.5 Use different knn models to fit the train data and analyze with empirical risk

This task is also about fitting train data with different k values. But estimate the result with empirical risk, in this case, cross entropy value. This is done with the following code:

```{r}
r_v <- 0
for (k in 1:30) {
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  s <- 0
  for (i in 0:9) {
    s <- s + sum(log2(kv$prob[which(validation$number==i), toString(i)] + 1e-15)/nrow(kv$prob))
  }
  r_v[k] <- -s/10
}
plot(1:30, r_v, ylim = c(0,0.14), type = "l", xlab = "K", ylab = "cross entropy")
```

The optimized K value is 6, because at that point, the risk value is the lowest with a value being 0.0172. The reason cross entropy is a better approach of estimating optimal model here is because it uses the probabilities of each class for every observation. While the mis-classification rate only count the final result class of each observation. Thus, cross-entropy can detect which model is better even though their mis-classification rates are the same.


## Question 2

Bayesian ridge regression model for the data (without the intercept parameter) is: $$\mathbf{y} | \mathbb{X}, w_0, \mathbf{w} \sim N(w_0 + \mathbb{X}\mathbf{w}, \, \sigma^2\mathbb{I}),$$
where:

*   $\mathbf{y}$ is the vector of Parkinson's disease symptom scores, "motor UPDRS", the dependent variable
*   $\mathbb{X}$ is the design matrix containing the observed values of all the independent variables corresponding to $\mathbf{y}$ (voice characteristics in this case)
*   $\boldsymbol{w}$ is the vector of bayesian ridge regression parameters
*   $\sigma$ is a scalar such that $\sigma^2 \cdot \mathbb{I}$ is the covariance matrix of the linear regression error terms

The Bayesian prior for $\boldsymbol{w}$ is: $$\mathbf{w} \sim N(0, \, \frac{\sigma^2}{\lambda}\mathbb{I})$$

Fitting the above mentioned ridge regression model on the training dataset, with $\lambda = 1, 100 and 1,000$, we see the training and test [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) values as shown in the plot below.\

``` {r, echo = FALSE, fig.asp = 0.5}
# read in data
parkinsons <- read_csv("parkinsons.csv", col_types = cols())

# drop columns not to be used for prediction
parkinsons <- parkinsons[, -c(1, 2, 3, 4, 6)]

# bayesian model

## $$\mathbf{y} | \mathbb{X}, w_0, \mathbf{w} \sim N(w_0 + \mathbb{X}\mathbf{w}, \sigma^2\cdot \mathbb{I})$$
## $$\mathbf{w} \sim N(0, \sigma^2/\lambda \cdot \mathbb{I})$$

# create train/test partitions
all_indices <- 1:(nrow(parkinsons))
set.seed(12345)
train_indices <- sample(all_indices, ceiling(0.6*nrow(parkinsons)))
test_indices <- all_indices[!(all_indices %in% train_indices)]
train_data <- parkinsons[train_indices, ]
test_data <- parkinsons[test_indices, ]

# scale data -- see https://sebastianraschka.com/faq/docs/scale-training-test.html for why training mu/sigma are used for scaling test data
n_col <- ncol(parkinsons)
train_mu <- sapply(1:n_col, function(x) mean(unlist(train_data[x])))
train_sigma <- sapply(1:n_col, function(x) sd(unlist(train_data[x])))

train_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (train_data[x] - train_mu[x])/train_sigma[x]))) # as.matrix has no effect without as.data.frame!!!
test_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (test_data[x] - train_mu[x])/train_sigma[x])))

# set up helper functions

model_log_likelihood <- function(w_vector, sigma, X) { # built for no intercept models, as asked
  
  n <- nrow(X)
  term1 <- -n/2*log(2*pi*sigma^2)
  term2 <- -sum(sapply(1:n, function(x) X[x, 1] - X[x, -1] %*% w_vector)^2)/2/sigma^2 # assumes y = X[, 1]
  
  return(term1 + term2)
  
}

ridge <- function(w_vector_sigma, X, lambda) { # built for no intercept models, as asked
  
  n <- ncol(X)
  sigma <- w_vector_sigma[n]
  w_vector <- w_vector_sigma[1:(n-1)]
  return(lambda*sum(w_vector^2) - model_log_likelihood(w_vector, sigma, X))
  
}

ridge_opt <- function(X, lambda) {
  
  n <- ncol(X)
  par <- rnorm(n-1) # initial parameter values (to seed optimisation); random
  par[n] <- 1 # initialise with positive value
  return(optim(par = par, fn = ridge, method = "BFGS", X = X, lambda = lambda)$par)
  
}

degrees_of_freedom <- function(X, num_samples, num_simulations, optimal_weigths_sigma) {
  
  n <- ncol(X)
  r <- nrow(X)
  optimal_weigths <- optimal_weigths_sigma[1:(n-1)] # assumes no intercept as asked
  optimal_sigma <- optimal_weigths_sigma[n]
  sum_cov = 0
  
  while (num_simulations > 0) {
    
    set.seed(num_simulations)
    sample_data <- X[sample(1:r, num_samples), ]
    sum_cov <- sum_cov + cov(sample_data[, 1], sample_data[, -1] %*% optimal_weigths)
    num_simulations = num_simulations - 1
    
  }
  
  return(as.numeric(sum_cov/optimal_sigma^2))
  
}

MSE <- function(X, optimal_weights) { # based on definition here: https://en.wikipedia.org/wiki/Mean_squared_error
  
  n <- nrow(X)
  return(sum((X[, 1] - X[, -1] %*% optimal_weights)^2)/n) # built for no-intercept models as asked
  
}

# model 1: lambda = 1
model1_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1)
model1_optimal_weights <- model1_optimal_weights_sigma[1:(n_col - 1)]
model1_optimal_sigma <- model1_optimal_weights_sigma[n_col]
model1_train_MSE <- MSE(train_data, model1_optimal_weights)
model1_test_MSE <- MSE(test_data, model1_optimal_weights)
model1_AIC <- -2*model_log_likelihood(model1_optimal_weights, model1_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), num_samples = 1000, 
                       num_simulations = 50, model1_optimal_weights_sigma)

# model 2: lambda = 100
model2_optimal_weights_sigma <- ridge_opt(train_data, lambda = 100)
model2_optimal_weights <- model2_optimal_weights_sigma[1:(n_col - 1)]
model2_optimal_sigma <- model2_optimal_weights_sigma[n_col]
model2_train_MSE <- MSE(train_data, model2_optimal_weights)
model2_test_MSE <- MSE(test_data, model2_optimal_weights)
model2_AIC <- -2*model_log_likelihood(model2_optimal_weights, model2_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), num_samples = 1000, 
                       num_simulations = 50, model2_optimal_weights_sigma)

# model 3: lambda = 1000
model3_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1000)
model3_optimal_weights <- model3_optimal_weights_sigma[1:(n_col - 1)]
model3_optimal_sigma <- model3_optimal_weights_sigma[n_col]
model3_train_MSE <- MSE(train_data, model3_optimal_weights)
model3_test_MSE <- MSE(test_data, model3_optimal_weights)
model3_AIC <- -2*model_log_likelihood(model3_optimal_weights, model3_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), num_samples = 1000, 
                       num_simulations = 50, model3_optimal_weights_sigma)

# plot performance
MSE_data <- data.frame(log_10_lambda = c(0, 0, 2, 2, 3, 3), Legend = rep(c("Training", "Test"), 3), 
                       value = c(model1_train_MSE, model1_test_MSE, model2_train_MSE, 
                                 model2_test_MSE, model3_train_MSE, model3_test_MSE))

ggplot(MSE_data) + geom_line(aes(log_10_lambda, value, colour = Legend)) +
  theme_bw() + geom_label(aes(log_10_lambda, value, label = round(value, 4))) + 
  geom_vline(xintercept = 2, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") + ylab("Mean Squared Error") + 
  ggtitle("Finding the Optimal Hyper-Parameter")
```

\  

We can see that the optimal value of $\lambda$ within the set $\{1, 100, 1000\}$ is 100. At $\lambda = 1$, there is overfitting, evidenced by low training error and large test error; at $\lambda = 1000$, on the other hand, there is underfitting, evidenced by high training and test error.\

MSE a more appropriate measure here than other empirical risk functions such as MAE, because in regression problems, unlike measures like MAE, MSE can also serve as an estimator of the variance of the error terms. Of course empirical risk functions such as mis-classification rate or cross-entropy do not apply in regression problems such as this, making MSE more appropriate than such measures.\

In terms of AIC as well, the optimal value of $\lambda$ within the set $\{1, 100, 1000\}$ is 100. One theoretical advantage of using AIC is that unlike the holdout method, it can be used for model selection in unsupervised learning models, where losses, unlike MSE, will not be a function of 'labels' and where one cannot ascertain under/overfitting based on test MSEs.\

``` {r, echo = FALSE, fig.asp = 0.5}
AIC_data <- data.frame(log_10_lambda = c(0, 2, 3), value = c(model1_AIC, model2_AIC, model3_AIC))

ggplot(AIC_data) + geom_line(aes(log_10_lambda, value)) + theme_bw() + 
  geom_vline(xintercept = 2, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") + ylab("Model AIC") + 
  ggtitle("Finding the Optimal Hyper-Parameter")
```

## Question 3

The linear regression model for the data is: $$\mathbf{y} \sim \mathcal{N}(\mathbb{X}\boldsymbol{\beta}, \, \sigma^2 \mathbb{I}),$$
where:

*   $\mathbf{y}$ is the vector "Fat" (the dependent variable)
*   $\mathbb{X}$ is the design matrix containing the observed values of all the independent variables corresponding to $\mathbf{y}$ (absorbance characteristics, in thtis case), prefixed with a column of 1s for the intercept
*   $\boldsymbol{\beta}$ is the vector true linear regression parameters
*   $\sigma$ is a scalar such that $\sigma^2\cdot \mathbb{I}$ is the covariance matrix of the linear regression error terms


Fitting the above mentioned model on the given dataset we see low training error and high test error; errors were measured using the mean-squared error loss function.

``` {r, echo = FALSE, fig.asp = 0.5}
tecator <- read_csv("tecator.csv", col_types = cols())

# drop columns not to be used for prediction
tecator <- tecator[, -c(1, 103, 104)]

# split into training/test
all_indices <- 1:(nrow(tecator))
set.seed(12345)
train_indices <- sample(all_indices, ceiling(0.5*nrow(tecator)))
test_indices <- all_indices[!(all_indices %in% train_indices)]
train_data <- tecator[train_indices, ]
test_data <- tecator[test_indices, ]

# linear regression
linreg <- lm(Fat~., train_data)
y_train_pred <- predict(linreg)
y_test_pred <- predict(linreg, test_data)
train_mse <- sum((train_data$Fat - y_train_pred)^2)/nrow(train_data)
test_mse <- sum((test_data$Fat - y_test_pred)^2)/nrow(test_data)

cat("Training MSE is:", train_mse)
cat("Test MSE is: ", test_mse)
```


These statistics indicate overfitting. On average training predictions are very close to actual training labels, but test predictions are relatively very different from test labels. As a result we cannot use satisfactorily use this model to predict fat content in any new meat sample.\

If we turn to the LASSO regression method, however, we would be selected $\boldsymbol{\beta}$ such that the following loss function is minimised: $$\sum\limits_{i=1}^N{(y_i - \mathbb{X}_i\boldsymbol{\beta})^2} + \lambda\sum\limits_{j=1}^p{|\beta_i|}$$
where, additionally:

*   $N$ is the total number of observations in the dataset
*   $i$ is an index the observations in the dataset, and runs from 1, 2, 3, ... $N$
*   $\lambda$ is the LASSO hyper-parameter


With $\lambda \in \{0.001, 0.01, 0.1, 1, 10, 100, 1000\}$, LASSO regression estimates parameters with attributes as shown in the plot below:\

``` {r, echo = FALSE, fig.asp = 0.5}
# helper function: degrees of freedom LASSO
lasso_degrees_of_freedom <- function(lasso_model, y, X, num_simulations, num_samples) {
  
  X <- as.matrix(X)
  y <- unlist(y)
  y_pred <- as.numeric(predict(lasso, X))
  r <- nrow(X)
  sum_cov = 0
  
  while (num_simulations > 0) {
    
    set.seed(num_simulations)
    sample_indices <- sample(1:r, num_samples)
    sample_X <- X[sample_indices, ]
    sample_y <- y[sample_indices]
    sample_y_pred <- y_pred[sample_indices]
    sum_cov <- sum_cov + cov(sample_y, sample_y_pred)
    num_simulations = num_simulations - 1
    
  }
  
  sigma_hat_square <- sum((y - y_pred)^2)/num_samples
  
  return(sum_cov/sigma_hat_square)
  
}

# fit LASSO
lambda_choices <- c(10^-3, 10^-2, 10^-1, 1, 10, 100, 1000)
zero_coeff <- c()
sum_abs_val_params <- c()
lasso_deg_freedoms <- c()
for (lam in lambda_choices){

  lasso <- glmnet(x = as.matrix(subset(train_data, select = -Fat)),
                  y = unlist(train_data[, "Fat"]), alpha = 1, lambda = lam)
  sum_abs_val_params <- c(sum_abs_val_params, sum(abs(lasso$beta)))
  zero_coeff <- c(zero_coeff, sum(lasso$beta == 0))
  lasso_deg_freedoms <- c(lasso_deg_freedoms, 
                          lasso_degrees_of_freedom(lasso, tecator[, "Fat"], subset(tecator, select = -Fat), 
                                                   num_simulations = 50, num_samples = 100)) # using full dataset rather than train / test, because why not!
  
}

# plot results LASSO
lasso_results <- data.frame(log_10_lambda = rep(c(-3, -2, -1, 0, 1, 2, 3), 2), 
                            Legend = c(rep("# Parameters with Coefficients = 0", 7), 
                                       rep("Sum of absolute values of Coefficients", 7)),
                            value = c(zero_coeff, sum_abs_val_params))

ggplot(lasso_results) + geom_line(aes(log_10_lambda, value, colour = Legend)) + theme_bw() + 
  scale_x_continuous(breaks = -3:3) + scale_y_continuous(breaks = seq(0, 1200, 100)) + 
  theme(legend.position="bottom") + 
  xlab("Log (base-10) of LASSO Hyper-Parameter 'lambda'") + ylab("Parameter Attributes") + 
  ggtitle("Relationship between LASSO Lambda and parameter attributes")

```

\  

What we see above, as expected in theory, is that increases in $\lambda$ result in increased penalty on the absolute values of parameter estimates. When $\lambda >= 1$, LASSO reduces to estimates of all 100 parameters to zero and brings down the value of the sum of absolute values of all parameters estimates to zero.\

The most appropriate value for lambda in a case where there are only 3 independent variables can, as is common, be estimated through cross-validation or holdout sample methods, but one should not expect the estimates to be very large.\

Plotting model degrees of freedom versus $\lambda$ on log (base 10) scale, we can see the former decrease as $\lambda$ increases; with very large values of lambda (such as $\lambda >= 100$ in this case), degrees of freedom will drop to zero, the lowest possible value for this measure.\

``` {r, echo = FALSE, fig.asp = 0.5}
lasso_deg_freedom <- data.frame(log_10_lambda = rep(c(-3, -2, -1, 0, 1, 2, 3), 2), value = lasso_deg_freedoms)

ggplot(lasso_deg_freedom) + geom_line(aes(log_10_lambda, value)) + theme_bw() +
  scale_x_continuous(breaks = -3:3) + xlab("Log (base-10) of LASSO Hyper-Parameter 'lambda'") + 
  ylab("Model Degrees of Freedom") + ggtitle("Relationship between LASSO Lambda and Model Degrees of Freedom")

```

With $\lambda \in \{0.001, 0.01, 0.1, 1, 10, 100, 1000\}$, ridge regression, on the other hand, estimates parameters with attributes as shown in the plot below:\

``` {r, echo = FALSE, fig.asp = 0.5}
# fit ridge
lambda_choices <- c(10^-3, 10^-2, 10^-1, 1, 10, 100, 1000)
zero_coeff <- c()
sum_params_squared <- c()
for (lam in lambda_choices){
  
  ridge <- glmnet(x = as.matrix(subset(train_data, select = -Fat)),
                  y = unlist(train_data[, "Fat"]), alpha = 0, lambda = lam)
  sum_params_squared <- c(sum_params_squared, sum(ridge$beta^2))
  zero_coeff <- c(zero_coeff, sum(ridge$beta == 0))

}

# plot results ridge
ridge_results <- data.frame(log_10_lambda = rep(c(-3, -2, -1, 0, 1, 2, 3), 2), 
                            Legend = c(rep("# Parameters with Coefficients = 0", 7), 
                                       rep("Sum of squared values of Coefficients", 7)),
                            value = c(zero_coeff, sum_params_squared))

ggplot(ridge_results) + geom_line(aes(log_10_lambda, value, colour = Legend)) + theme_bw() + 
  scale_x_continuous(breaks = -3:3) + theme(legend.position="bottom") + 
  xlab("Log (base-10) of Ridge Hyper-Parameter 'lambda'") + ylab("Parameter Attributes") + 
  ggtitle("Relationship between Ridge Lambda and parameter attributes")
```

\  

Upon comparison, it becomes clear that ridge regression, unlike LASSO, does not bring down any parameter estimates to zero.\

Upon running 3-fold cross-validation on the training dataset, we can see that CV score increases as $\lambda$ increases.\
``` {r, echo = FALSE, fig.asp = 0.5}
# LASSO CV
cv_means <- c()
cv_min_lambda <- c()
for (i in 1:100) {
  
  cv_lasso <- cv.glmnet(x = as.matrix(subset(train_data, select = -Fat)), 
                        y = unlist(train_data[, "Fat"]), alpha = 1, 
                        lambda = lambda_choices, nfolds = 3)
  cv_means <- cbind(cv_means, cv_lasso$cvm)
  cv_min_lambda <- c(cv_min_lambda, cv_lasso$lambda.min)
}

# plot LASSO CV results
ggplot(data.frame(log_10_lambda = -3:3, cv_score = rev(rowMeans(cv_means)))) + 
  geom_line(aes(log_10_lambda, cv_score)) + scale_x_continuous(breaks = -3:3) +
  geom_vline(xintercept = -3, linetype = "dotted") + 
  xlab("Log (base-10) of LASSO Hyper-Parameter 'lambda'") + ylab("Mean squared error across CV folds") + 
  ggtitle("Relationship between LASSO Lambda and CV Scores")

opt_lambda <- cv_lasso$lambda.min
cat("Optimal Lambda is:", opt_lambda)

model_optimal_lambda <- glmnet(x = as.matrix(subset(train_data, select = -Fat)),
                               y = unlist(train_data[, "Fat"]), alpha = 1, lambda = opt_lambda)
num_non_zero_params_opt_lambda <- sum(model_optimal_lambda$beta != 0)
cat("Number of variables chosen in the model:", num_non_zero_params_opt_lambda)

# comparing LASSO regression model with optimal lambda and another with log (base-10) lambda = -2
model_log_lambda_minus_2 <- glmnet(x = as.matrix(subset(train_data, select = -Fat)),
                                   y = unlist(train_data[, "Fat"]), alpha = 1, lambda = 10^-2)
y_test <- unlist(test_data[, "Fat"])
y_test_pred_opt_lambda <- predict(model_optimal_lambda, as.matrix(subset(test_data, select = -Fat)))
y_test_log_lambda_minus_2 <- predict(model_log_lambda_minus_2, as.matrix(subset(test_data, select = -Fat)))

num_non_zero_params_log_lambda_minus_2 <- sum(model_log_lambda_minus_2$beta != 0)

opt_lambda_SSE <- sum((y_test - y_test_pred_opt_lambda)^2) 
log_lambda_minus_2_SSE <- sum((y_test - y_test_log_lambda_minus_2)^2)

opt_lambda_deg_freedom <- nrow(test_data) - num_non_zero_params_opt_lambda - 1
log_lambda_minus_2_deg_freedom <- nrow(test_data) - num_non_zero_params_log_lambda_minus_2 - 1

opt_lambda_MSE <- opt_lambda_SSE / opt_lambda_deg_freedom
log_lambda_minus_2_MSE <- log_lambda_minus_2_SSE / log_lambda_minus_2_deg_freedom
test_statistic <- opt_lambda_MSE / log_lambda_minus_2_MSE

# Null Hypothesis: opt_lambda_MSE = log_lambda_minus_2_MSE
# Alternative Hypothesis: opt_lambda_MSE < log_lambda_minus_2_MSE

p_value <- pf(q = test_statistic, 
              df1 = opt_lambda_deg_freedom, 
              df2 = log_lambda_minus_2_deg_freedom)

cat("The p-value for the test of null hypothesis that optimal lambda works similar to log(lambda) = -2",
    "vs. the alternative that the former is better is:", p_value)
cat("We, therefore, cannot conclude confidently that the optimal",
    "lambda works significantly better than log(lambda) = -2")

# plot predictions LASSO CV Optimal
ggplot(data.frame(y_test, y_test_pred_opt_lambda)) + 
  geom_point(aes(y_test, y_test_pred_opt_lambda)) + 
  xlab("Test Labels") + ylab("Test Predictions") +
  ggtitle("Goodness of fit - LASSO model with optimised lambda")
```

\  

The LASSO regression model with optimised $\lambda = 10^-3$ produces good test predictions which are concentrated around the $y = x$ line in the plot of Predictions vs. Actual Labels.\

The same, however, is not a good generative model, because of the assumption that the training dataset is deterministic. This can also be seen in the plot below, where points generated from the $\mathcal{N}(\mathbb{X}\hat{\beta}_{LASSO}, \hat{\sigma}_{MLE}^2)$ are quite spread out around the $y = x$ line (relative to the spread seen in the non-generative, deterministic model).

``` {r, echo = FALSE, fig.asp = 0.5}
# estimate sigma: use MLE now ;-)
y_train <- unlist(train_data[, "Fat"])
y_train_pred <- predict(model_optimal_lambda, as.matrix(subset(train_data, select = -Fat)))
MLE_sigma_estimate <- sqrt(sum((y_train - y_train_pred)^2)/nrow(train_data))

# generate labels from distribution N(y_train_pred, MLE_sigma_estimate*I)
y_generated <- rnorm(length(y_test), y_test_pred_opt_lambda, MLE_sigma_estimate)

# plot generated labels vs. original labels
ggplot(data.frame(y_test, y_generated)) + 
  geom_point(aes(y_test, y_generated)) + 
  xlab("Test Labels") + ylab("Generated Labels") +
  ggtitle("Goodness of fit - Generative LASSO model with optimised lambda")
```




# Assignment 3: Linear regression and LASSO

### 3.1
```{r, echo=FALSE}
fats <- read.csv("tecator.csv", row.names = 1)
fats <- fats[, -c(102,103)]
set.seed(12345)
n3 <- nrow(fats)
id_fat_train <- sample(1:n3, floor(n3*0.5))
id_fat_test <- setdiff(1:n3, id_fat_train)

train_fat <- fats[id_fat_train,]
test_fat <- fats[id_fat_test,]
```



```{r,echo =FALSE}


train_fat <- as.data.frame(train_fat)
test_fat<-as.data.frame(test_fat)
lm_fats_train <- lm(Fat ~., data = train_fat )

sum_mary<-summary(lm_fats_train)
#predictions 
predicted_train_fat <- predict(lm_fats_train)

#train MSE
mse_train <- mean((train_fat$Fat - predicted_train_fat)^2)

predicted_test_fat<-predict(lm_fats_train, test_fat)

#test MSE
mse_test <- mean((test_fat$Fat- predicted_test_fat)^2)

cat("Training MSE: ", mse_train, ". Test MSE: ", mse_test,"\n")
cat("Adjusted R-square: ", sum_mary$adj.r.squared)

```



With the very low training MSE and comparably high test MSE we have reason to believe that there is overfitting occurring with the model. From viewing the summary for the training model it was found to give a adjusted R-square of 0.9994% which gives an clear indication that the model is overfit.

```{r, echo=FALSE}
par(mfrow = c(2,2))

plot(lm_fats_train)

```



Analyzing the plots we find that the assumptions that the model residuals are i.i.d can be assumed to be approximately fullfilled, it's clear that the residuals do not follow anything similar to a normal distribution with mean 0 and variance 1 when observing the QQ-plot. Hence the assumptions for the model are not fulfilled so we should not draw any conclusions from the model.



### 3.2

The function we are looking to optimize in this case is loss function of the LASSO model.


$$\sum\limits_{i=1}^N{(y_i - \mathbb{X}_i\boldsymbol{\beta})^2} + \lambda\sum\limits_{j=1}^p{|\beta_j|}$$

Where $N$ is the number of observations and $\mathbb{X}$ is the matrix of features and $y$ is the dependent variable. $\beta$ are the parameters that the shrinkage function right part of the plus sign. With $\lambda$ being the shrinkage parameter and P the number of features. Where the chosen $\lambda$ will shrink the $\beta$ parameters. 

### 3.3
```{r, echo=FALSE}



x<-as.matrix(train_fat[,c(1:100)])
y <- as.matrix(train_fat[,101])


lambda_values<-seq(from = 0.01, to = 1, by = 0.002)
lambda_values<-rev(lambda_values)
lasso3_3_df <- 0
i <- 1

#Finding where we get 3 parameters numerically
while(lasso3_3_df < 3){

lasso3_3 <- glmnet(x = x, y = y, alpha = 1, lambda = lambda_values[i])

lasso3_3_df <-   lasso3_3$df
i = i+1
}

log(lambda_values[i])

lasso3_3_for_plot<-glmnet(x = x, y = y, alpha = 1)

plot(lasso3_3_for_plot, "lambda" , main = "LASSO model coefficent convergence")


```

From the plot we can see that where the function converges to 3 features that have non zero coefficients is approximately at $log(\lambda) = -0.2$ and it was numerically shown that the upper bound approximately to be at $log(\lambda) = -0.1031408$ where the function would have a df of 3 and 3 features with non zero coefficients. Approximately between the values  $log(\lambda) = (-0.5,-0.1031408)$ the model will have 3 features. 

### 3.4

```{r, echo=FALSE}

plot_data <- data.frame("log_lambda" = log(lasso3_3_for_plot$lambda), "df" = lasso3_3_for_plot$df )
ggplot(data = plot_data ,aes( x= log_lambda , y = df)) + geom_point() + 
  labs(title = "Degrees of freedom vs Log lambda", x = "Log lambda", y = "Degrees of freedom")

```

The graph shows that as the value of the shrinkage parameter lambda increases it reduces the number of features in the model. Which is exactly as expected from the given function with an sufficiently large $\lambda$ it will reduce the number of features included to 0 as such decreasing the degrees of freedom. 

```{r, echo=FALSE}
lasso3_5_for_plot<-glmnet(x = x, y = y, alpha = 0)

plot(lasso3_3_for_plot, "lambda", main = "Lasso regression: Coef vs log lambda")
plot(lasso3_5_for_plot, "lambda", main = "Ridge regression: Coef vs log Lambda")
```

As seen in the graphs the lasso function will converge the coefficients of the model to 0 whereas the ridge regression model will reduce the coefficients close to zero but never to zero.

```{r, echo=FALSE}
# cross validation with 10 folds
set.seed(12345)
cv_lasso <- cv.glmnet(x = x, y = y, alpha = 1)
min_lambda<-as.numeric(which(cv_lasso$lambda.min==cv_lasso$lambda))
point_upper_limit_of_CI<-cv_lasso$cvup[min_lambda]
plot(cv_lasso, main = "Cross-validation 10 fold CI:s for MSE vs values of lambda")
abline(h = point_upper_limit_of_CI)



```
From the cross-validation plot we can easier differentiate with the help of the black horizontal line which goes from the top confidence interval for the mean-square error for the optimal lambda that it crosses within the confidence interval for $log(\lambda) = -2$. Which means that we cannot say that the optimal model is significantly better than the $log(\lambda) = -2$ model.

The graph shows that the lowest MSE is found at $\lambda = 0.05234206$ which is at $log(\lambda) = -2.949955$ as shown in the graph. As the $log(\lambda)$ increases the MSE will increase with a linear rate from -8 to approx -5 and then with a non-linear increase until approximately $log(\lambda)$ reaches 0 and then where it will slowdown the increasing rate of MSE.

```{r, echo=FALSE}


lasso3_6_opt <- glmnet(x = x, y = y, alpha = 1, lambda = cv_lasso$lambda.min)

lasso3_6_opt$df
#We find that 7 variables were used in the 

lambda_test<-exp(-2)
lasso3_6_lambda_chosen <- glmnet(x = x, y = y, alpha = 1, lambda = lambda_test)

lasso3_6_lambda_chosen$df


```


```{r, echo = FALSE}

#plotting predicted vs observed values

y_hat_lasso <-predict(lasso3_6_opt, as.matrix(test_fat[,-101]))

plot_data_36 <- data.frame("y_test" = test_fat[,101] , "ylassopred" = y_hat_lasso)

ggplot(plot_data_36, aes(x = y_test, y = s0)) + geom_point() + labs(title = "Comparison of Predicted values with LASSO optimal vs \n observed values from the test data", x = "y from test data", y = "Predicted values from LASSO")
```

From observing the plot we can say that the predictions are a decent estimate as a perfect fit would results in a diagonal line over the graph. There seems to be a approximately linear trend between the predictions and true values. The model seem to over estimate for high values


### 3.7

```{r}
#Predicting from lasso lambda opt

preds <- predict(lasso3_6_opt,as.matrix( test_fat[,-101]))

#calculating new sigma
sigma<- sqrt(mean((train_fat[,101] - predict(lasso3_6_opt,as.matrix(train_fat[,-101])))^2 ))

#generating new data
new_targets <- rnorm(n = nrow(test_fat),preds, sigma )

generated_data <- data.frame("ytest" = test_fat[,101], "generated" = new_targets )
ggplot(generated_data, aes(x = ytest, y = generated)) + geom_point()+
  labs(title = "Generated data vs observed test data", x = "test y", y = "Generated data")



```

The generation seems to generate decent approximation of the new target values as we can see some spread in the values around the diagonal, it seems to generate slightly higher values on average than the observed ones. 
















## Appendix: Code

``` {r, eval = FALSE}
library(readr)
library(kknn)
library(ggplot2)
library(data.table)
library(glmnet)

#Assignment 1
#1.1
digits <- read.csv("optdigits.csv", header = F)
colnames(digits)[ncol(digits)] <- "number"
digits$number <- as.factor(digits$number)
n <- dim(digits)[1] 
set.seed(12345)
id <- sample(1:n, floor(n*0.5)) 
train <- digits[id,]
id1 <- setdiff(1:n, id) 
set.seed(12345) 
id2 <- sample(id1, floor(n*0.25)) 
validation <- digits[id2,]
id3 <- setdiff(id1,id2) 
test <- digits[id3,]

#1.2
knn_train <- kknn(number ~., train, train, k = 30, kernel = "rectangular")
knn_test <- kknn(number ~., train, test, k = 30, kernel = "rectangular")
#Confusion matrix
pred_train<- fitted(knn_train)
confusion_train <- table(train$number, pred_train)
pred_test <- fitted(knn_test)
confusion_test <- table(test$number, pred_test)
print(confusion_train)
print(confusion_test)
print(diag(confusion_train)/rowSums(confusion_train))
print(diag(confusion_test)/rowSums(confusion_test))
#Misclassification errors
missclass <- function(X,X1) { 
  n = length(X)
  return(1-sum(diag(table(X,X1)))/n)
}
mismatch_rate_train <- missclass(train$number, pred_train) #0.04500262
mismatch_rate_test <- missclass(test$number, pred_test) #0.05329154
cat("The mismatch rate for train data is:", mismatch_rate_train, 
    "\nThe mismatch rate for test data is:", mismatch_rate_test)

#1.3
train_eight <- train[which(train$number==8),]
train_eight$prob <- knn_train$prob[which(train$number==8), "8"]
sortedResult <- sort(train_eight[, "prob"], index.return = T)
sortedResult$x[1:3] #0.1000000 0.1333333 0.1666667
hard_ids <- sortedResult$ix[1:3] #50  43 136
tail(sortedResult$x, n=c(2)) #1 1
easy_ids <- tail(sortedResult$ix, n=c(2)) #179 183
plotHeatmap <- function(index){
  heatmap(matrix(as.numeric(train_eight[index, 1:64]), 8, 8, byrow = T), Colv = NA, Rowv = NA)
}
for (i in c(hard_ids, easy_ids)) {
  plotHeatmap(i)
}

#1.4
e_t <- 0
e_v <- 0
for (k in 1:30) {
  kt <- kknn(number ~., train, train, k = k, kernel = "rectangular")
  e_t[k] <- missclass(train$number, fitted(kt))
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  e_v[k] <- missclass(validation$number, fitted(kv))
}
plot(1:30, e_v, ylim = c(0,0.06), type = "l", col = "red", xlab = "K", ylab = "miss-classification error rate")
lines(1:30, e_t, ylim = c(0,0.06), col = "blue")
legend(2, 0.06, legend=c("validation", "train"),
       col=c("red", "blue"), lty=1:1, cex=0.8)

k_test_optik <- kknn(number ~., train, test, k = 3, kernel = "rectangular")
cat("Test mis-classification rate when k=3 is:", missclass(test$number, fitted(k_test_optik)))
cat("Train mis-classification rate when k=3 is:", e_t[3])
cat("Validation mis-classification rate when k=3 is:", e_v[3])

#1.5
r_v <- 0
for (k in 1:30) {
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  s <- 0
  for (i in 0:9) {
    s <- s + sum(log2(kv$prob[which(validation$number==i), toString(i)] + 1e-15)/nrow(kv$prob))
  }
  r_v[k] <- -s/10
}
plot(1:30, r_v, ylim = c(0,0.14), type = "l", xlab = "K", ylab = "cross entropy")


### Question 2
# read in data
parkinsons <- read_csv("parkinsons.csv", col_types = cols())

# drop columns not to be used for prediction
parkinsons <- parkinsons[, -c(1, 2, 3, 4, 6)]

# bayesian model

## $$\mathbf{y} | \mathbb{X}, w_0, \mathbf{w} \sim N(w_0 + \mathbb{X}\mathbf{w}, \, 
## \sigma^2 \mathbb{I})$$
## $\boldsymbol{w}$ is: $$\mathbf{w} \sim N(0, \, \frac{\sigma^2}{\lambda}
## \mathbb{I})$$

# create train/test partitions
all_indices <- 1:(nrow(parkinsons))
set.seed(12345)
train_indices <- sample(all_indices, ceiling(0.6*nrow(parkinsons)))
test_indices <- all_indices[!(all_indices %in% train_indices)]
train_data <- parkinsons[train_indices, ]
test_data <- parkinsons[test_indices, ]

# scale data -- see https://sebastianraschka.com/faq/docs/scale-training-test.html
# for why training mu/sigma are used for scaling test data
n_col <- ncol(parkinsons)
train_mu <- sapply(1:n_col, function(x) mean(unlist(train_data[x])))
train_sigma <- sapply(1:n_col, function(x) sd(unlist(train_data[x])))

train_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (train_data[x] - train_mu[x])/train_sigma[x])))
# as.matrix has no effect without as.data.frame!!!
test_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (test_data[x] - train_mu[x])/train_sigma[x])))

# set up helper functions

model_log_likelihood <- function(w_vector, sigma, X) { # built for no intercept
  # models, as asked
  
  n <- nrow(X)
  term1 <- -n/2*log(2*pi*sigma^2)
  term2 <- -sum(sapply(1:n, function(x) X[x, 1] - X[x, -1] %*% w_vector)^2)/
    2/sigma^2 # assumes y = X[, 1]
  
  return(term1 + term2)
  
}

ridge <- function(w_vector_sigma, X, lambda) { # built for no intercept models
  # as asked
  
  n <- ncol(X)
  sigma <- w_vector_sigma[n]
  w_vector <- w_vector_sigma[1:(n-1)]
  return(lambda*sum(w_vector^2) - model_log_likelihood(w_vector, sigma, X))
  
}

ridge_opt <- function(X, lambda) {
  
  n <- ncol(X)
  par <- rnorm(n-1) # initial parameter values (to seed optimisation); random
  par[n] <- 1 # initialise with positive value
  return(optim(par = par, fn = ridge, method = "BFGS", X = X, lambda = lambda)$par)
  
}

degrees_of_freedom <- function(X, num_samples, 
                               num_simulations, optimal_weigths_sigma) {
  
  n <- ncol(X)
  r <- nrow(X)
  optimal_weigths <- optimal_weigths_sigma[1:(n-1)] # assumes no intercept
  optimal_sigma <- optimal_weigths_sigma[n]
  sum_cov = 0
  
  while (num_simulations > 0) {
    
    set.seed(num_simulations)
    sample_data <- X[sample(1:r, num_samples), ]
    sum_cov <- sum_cov + cov(sample_data[, 1], 
                             sample_data[, -1] %*% optimal_weigths)
    num_simulations = num_simulations - 1
    
  }
  
  return(as.numeric(sum_cov/optimal_sigma^2))
  
}

MSE <- function(X, optimal_weights) { # based on definition here: https://en.wikipedia.org/wiki/Mean_squared_error
  
  n <- nrow(X)
  return(sum((X[, 1] - X[, -1] %*% optimal_weights)^2)/n) # built for
  # no-intercept models as asked
  
}

# model 1: lambda = 1
model1_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1)
model1_optimal_weights <- model1_optimal_weights_sigma[1:(n_col - 1)]
model1_optimal_sigma <- model1_optimal_weights_sigma[n_col]
model1_train_MSE <- MSE(train_data, model1_optimal_weights)
model1_test_MSE <- MSE(test_data, model1_optimal_weights)
model1_AIC <- -2*model_log_likelihood(model1_optimal_weights, model1_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), num_samples = 1000, 
                       num_simulations = 50, model1_optimal_weights_sigma)

# model 2: lambda = 100
model2_optimal_weights_sigma <- ridge_opt(train_data, lambda = 100)
model2_optimal_weights <- model2_optimal_weights_sigma[1:(n_col - 1)]
model2_optimal_sigma <- model2_optimal_weights_sigma[n_col]
model2_train_MSE <- MSE(train_data, model2_optimal_weights)
model2_test_MSE <- MSE(test_data, model2_optimal_weights)
model2_AIC <- -2*model_log_likelihood(model2_optimal_weights, model2_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), num_samples = 1000, 
                       num_simulations = 50, model2_optimal_weights_sigma)

# model 3: lambda = 1000
model3_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1000)
model3_optimal_weights <- model3_optimal_weights_sigma[1:(n_col - 1)]
model3_optimal_sigma <- model3_optimal_weights_sigma[n_col]
model3_train_MSE <- MSE(train_data, model3_optimal_weights)
model3_test_MSE <- MSE(test_data, model3_optimal_weights)
model3_AIC <- -2*model_log_likelihood(model3_optimal_weights, model3_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), num_samples = 1000, 
                       num_simulations = 50, model3_optimal_weights_sigma)

# plot performance
MSE_data <- data.frame(log_10_lambda = c(0, 0, 2, 2, 3, 3), 
                       Legend = rep(c("Training", "Test"), 3), 
                       value = c(model1_train_MSE, model1_test_MSE,
                                 model2_train_MSE, model2_test_MSE,
                                 model3_train_MSE, model3_test_MSE))

ggplot(MSE_data) + geom_line(aes(log_10_lambda, value, colour = Legend) +
  theme_bw() + 
  geom_vline(xintercept = 2, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") +
  ylab("Mean Squared Error") + 
  ggtitle("Finding the Optimal Hyper-Parameter")

AIC_data <- data.frame(log_10_lambda = c(0, 2, 3), 
                       value = c(model1_AIC, model2_AIC, model3_AIC))

ggplot(AIC_data) + geom_line(aes(log_10_lambda, value)) + theme_bw() + 
  geom_vline(xintercept = 2, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") + ylab("Model AIC") + 
  ggtitle("Finding the Optimal Hyper-Parameter")


### Question 3


#Predicting from lasso lambda opt

preds <- predict(lasso3_6_opt,as.matrix( test_fat[,-101]))

#calculating new sigma
sigma<- sqrt(mean((train_fat[,101] - predict(lasso3_6_opt,as.matrix(train_fat[,-101])))^2 ))

#generating new data
new_targets <- rnorm(n = nrow(test_fat),preds, sigma )

generated_data <- data.frame("ytest" = test_fat[,101], "generated" = new_targets )
ggplot(generated_data, aes(x = ytest, y = generated)) + geom_point()+
  labs(title = "Generated data vs observed test data", x = "test y", y = "Generated data")




```