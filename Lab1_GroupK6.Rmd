---
title: "Machine Learning: Block 1 Lab 1 (Group K6)"
author: "Yiran Wang, Tore Andersson and Shashi Nagarajan Iyer"
date: "11/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(kknn)
library(ggplot2)
library(data.table)
library(glmnet)
```

## Statement of Contribution:
All assignments were completed independently by each team member and approaches/results were discussed within the group. Reports for assignments 1, 2 and 3 were produced primarily by Yiran, Shashi and Tore respectively.

## 1. Assignment 1 - Handwritten digit recognition with K-NN

### 1.1 Import data and divide it into training, validation and test sets

The first task is to import digit data and divide it into three sets for training, testing and validation respectively with 50%, 25% and 25% of the data. We use sample() with seed set as 12345.

```{r, echo = FALSE}
digits <- read.csv("optdigits.csv", header = F)
colnames(digits)[ncol(digits)] <- "number"
digits$number <- as.factor(digits$number)
n <- dim(digits)[1] 
set.seed(12345)
id <- sample(1:n, floor(n*0.5)) 
train <- digits[id,]
id1 <- setdiff(1:n, id) 
set.seed(12345) 
id2 <- sample(id1, floor(n*0.25)) 
validation <- digits[id2,]
id3 <- setdiff(id1,id2) 
test <- digits[id3,]
```


### 1.2 Train KNN model and estimate the result

This task consists of training a knn model with 30 nearest neighbors and estimate the result with confusion metrics and misclassification errors for both the training and test data. 

```{r, echo = FALSE}
knn_train <- kknn(number ~., train, train, k = 30, kernel = "rectangular")
knn_test <- kknn(number ~., train, test, k = 30, kernel = "rectangular")
pred_train<- fitted(knn_train)
confusion_train <- table(train$number, pred_train)
pred_test <- fitted(knn_test)
confusion_test <- table(test$number, pred_test)
```

The confusion matrix for train data is:

```{r, warning=FALSE, echo = FALSE}
print(confusion_train)
```

The confusion matrix for test data is:

```{r, warning=FALSE, echo = FALSE}
print(confusion_test)
```

With these two matrix, we can simply have the accuracy rate based on correct numbers over total numbers for each digits:

```{r, warning=FALSE, echo = FALSE}
print(diag(confusion_train)/rowSums(confusion_train))
print(diag(confusion_test)/rowSums(confusion_test))
```

For train data, the quality of predicting different digits is with a large range between 91% and 100%. The prediction for digit 0 is 100%, and following with 6, 2, 3 and 7 with 97% to 99% rate. While the lower ones are for digits 4, 9 and 1 with about 91% correct rate. This could indicate that for some classes with 91% accuracy rate in the train data, the model does not fit quite well.

For test data, the accuracy rates for prediction on digits do not have a huge difference than for train data. Some of them are higher than the train data prediction rate such as for 1, 6, 7 and 9. The highest one is 100% for digit 6. And other ones are below the train ones with a lowest rate for digit 4 being 86%. As we can see here, the rates for digits comparing with them for train data do not have a big difference, which means the model generalizes well and not over-fitting.

We can also have the mis-classification rate calculated as below:

```{r, warning=FALSE, echo = FALSE}
missclass <- function(X,X1) { 
  n = length(X)
  return(1-sum(diag(table(X,X1)))/n)
}
mismatch_rate_train <- missclass(train$number, pred_train)
mismatch_rate_test <- missclass(test$number, pred_test)
cat("The mismatch rate for train data is:", mismatch_rate_train, 
    "\nThe mismatch rate for test data is:", mismatch_rate_test)
```

For the overall mismatch rate 0.045 for train data and 0.053 for test data, we can see that the rates for both of them are fairly low and the difference between them is also not high. This can mean that the model is decent. But for a digit classification case with only 10 numbers, 5% error rate could mean that 1 in 20 observations would be classified wrong, which seems to be not good enough.


### 1.3 Identify and plot cases with low and high correct rates

This task is to find cases that are hard to classify and easy to classify and plot them with heatmap plots:

```{r, echo = FALSE}
train_eight <- train[which(train$number==8),]
train_eight$prob <- knn_train$prob[which(train$number==8), "8"]
sortedResult <- sort(train_eight[, "prob"], index.return = T)
#sortedResult$x[1:3] #0.1000000 0.1333333 0.1666667
hard_ids <- sortedResult$ix[1:3] #50  43 136
#tail(sortedResult$x, n=c(2)) #1 1
easy_ids <- tail(sortedResult$ix, n=c(2)) #179 183
plotHeatmap <- function(index){
  heatmap(matrix(as.numeric(train_eight[index, 1:64]), 8, 8, byrow = T), Colv = NA, Rowv = NA)
}
```
![](Lab1_assignment1_3.jpeg) 

The cases that are hard to see are chosen with lowest correct rates being 0.1000000, 0.1333333 and 0.1666667. While for the easy to identify ones, the rates are both 100%. As the heatmap shows, the cases that has lowest accuracy rates(the top three showing on the image) are hard to recognized as digits 8 when visualizing them as well. All of them are either too blur or the shape is not clear enough to identify as digit 8. While the 100% accurate cases(the bottom two on the image) are very easy to see as 8.


### 1.4 Use different knn models to fit the train data and analyze with miss-classification rate

This is about fitting the train data with K=1..30 knn models and plot the mis-classification rates for train and validation data. Then observe the plot in different perspective. The plot is shown as below: 

```{r, warning=FALSE, echo = FALSE}
e_t <- 0
e_v <- 0
for (k in 1:30) {
  kt <- kknn(number ~., train, train, k = k, kernel = "rectangular")
  e_t[k] <- missclass(train$number, fitted(kt))
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  e_v[k] <- missclass(validation$number, fitted(kv))
}
plot(1:30, e_v, ylim = c(0,0.06), type = "l", col = "red", xlab = "K", ylab = "miss-classification error rate")
lines(1:30, e_t, ylim = c(0,0.06), col = "blue")
legend(2, 0.06, legend=c("validation", "train"),
       col=c("red", "blue"), lty=1:1, cex=0.8)
```

As k increases, the complexity of the model for both data sets also increases overall. As we can see in the plot, the error rate for the validation data decreases first to 0.025 when K is 3 and stays a bit before going up quickly as K increases with a final slight drop till 0.05. While the error rate for the train data starts with 0 when k is 0, and then goes up gradually as K goes up.

The optimal K is a little difficult to detect right away since the differences for different K values are very small, especially when K is about 3 to 7. But we take 4 as the optimal one based on only the rate we plot here. As at that point, the value for validation rate is the lowest. The difference between validation and train data is not the smallest but also not very high. From the perspective of bias-variance trade-off, the difference could be interpreted as variance and the error rate can be seen as bias. In this case, we want the bias to be as small as possible and compromise with the variance not being perfect. Low bias is not always perfect as it can lead to a higher variance(when error of train data is 0, the error of validation data is relatively higher). When bias is a little bit higher, the variance can be a bit lower as it can generalize new data better. But after certain point, when bias gets too high, the error rate for both train and validation data will be high, which would be not a suitable model as it may be under-fitting.

With the optimal K value selected based on above analysis, the result of the model for test data can be shown as:

```{r, warning=FALSE, echo = FALSE}
k_test_optik <- kknn(number ~., train, test, k = 4, kernel = "rectangular")
cat("Test mis-classification rate when k=4 is:", missclass(test$number, fitted(k_test_optik)),
    "\nTrain mis-classification rate when k=4 is:", e_t[4],
    "\nValidation mis-classification rate when k=4 is:", e_v[4])
```

The test error being 2.51% with K=4 is about the same as the error rate of validation data and a little higher (about 1.15%) higher than train data. We think the quality of the model with K=4 is acceptable because with a similar relatively high fitting rate to both validation and test data(not over-fitting) and well fitted for train data(not under-fitting).


### 1.5 Use different knn models to fit the train data and analyze with empirical risk

This task is also about fitting train data with different k values. But estimate the result with empirical risk, in this case, cross entropy value. This plot of the result is as follows:

```{r, echo = FALSE}
r_v <- 0
for (k in 1:30) {
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  s <- 0
  for (i in 0:9) {
    s <- s + sum(log(kv$prob[which(validation$number==i), toString(i)] + 1e-15)/nrow(kv$prob))
  }
  r_v[k] <- -s
}
plot(1:30, r_v, type = "l", xlab = "K", ylab = "mean cross entropy")
```

As the plot shows, the optimized K value is 6, because at that point, the risk value is the lowest with a value being 0.1191288. The reason cross entropy is a better approach of estimating optimal model here is because it uses the probabilities of each class for every observation. The lower the the probability the model has for the correct class, the more penalty it will get. But for the mis-classification method, it only outputs 0 or 1, which means that even if the probabilities are slightly different or huge differ from each other, the penalty would be the same. Thus, the way cross-entropy works can lead the model to be trained more correctly.


## 2. Assignment 2 - Ridge regression and model selection

### 2.1-3 Bayesian Ridge regression model, Data Preparation and Functions to run Ridge regression

Bayesian ridge regression model for the data (without the intercept parameter) is: $$\mathbf{y} | \mathbb{X}, w_0, \mathbf{w} \sim N(w_0 + \mathbb{X}\mathbf{w}, \, \sigma^2\mathbb{I}),$$
where:

*   $\mathbf{y}$ is the vector of Parkinson's disease symptom scores, "motor UPDRS", the dependent variable
*   $\mathbb{X}$ is the design matrix containing the observed values of all the independent variables corresponding to $\mathbf{y}$ (voice characteristics in this case)
*   $\boldsymbol{w}$ is the vector of bayesian ridge regression parameters
*   $\sigma$ is a scalar such that $\sigma^2 \cdot \mathbb{I}$ is the covariance matrix of the linear regression error terms

The Bayesian prior for $\boldsymbol{w}$ is: $$\mathbf{w} \sim N(0, \, \frac{\sigma^2}{\lambda}\mathbb{I}),$$

where:

*   $\lambda$ is the ridge penalty hyper-parameter

Data and functions were prepared as required (see appendix for code)

### 2.4.1 Training and test MSE values

Fitting the above mentioned ridge regression model on the training dataset, with $\lambda = 1, 100 and 1,000$, we see the training and test [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) values as shown in the plot below. Note that the test data was scaled using the same means and standard deviations used to scale training data (this is as recommended [here](https://sebastianraschka.com/faq/docs/scale-training-test.html))

``` {r, echo = FALSE, fig.asp = 0.5}
# read in data
parkinsons <- read_csv("parkinsons.csv", col_types = cols())

# drop columns not to be used for prediction
parkinsons <- parkinsons[, -c(1, 2, 3, 4, 6)]

# bayesian model

## $$\mathbf{y} | \mathbb{X}, w_0, \mathbf{w} \sim N(w_0 + \mathbb{X}\mathbf{w}, \sigma^2\cdot \mathbb{I})$$
## $$\mathbf{w} \sim N(0, \sigma^2/\lambda \cdot \mathbb{I})$$

# create train/test partitions
all_indices <- 1:(nrow(parkinsons))
set.seed(12345)
train_indices <- sample(all_indices, ceiling(0.6*nrow(parkinsons)))
test_indices <- all_indices[!(all_indices %in% train_indices)]
train_data <- parkinsons[train_indices, ]
test_data <- parkinsons[test_indices, ]

# scale data -- see https://sebastianraschka.com/faq/docs/scale-training-test.html for why training mu/sigma are used for scaling test data
n_col <- ncol(parkinsons)
train_mu <- sapply(1:n_col, function(x) mean(unlist(train_data[x])))
train_sigma <- sapply(1:n_col, function(x) sd(unlist(train_data[x])))

train_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (train_data[x] - train_mu[x])/train_sigma[x]))) # as.matrix has no effect without as.data.frame!!!
test_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (test_data[x] - train_mu[x])/train_sigma[x])))

# set up helper functions

model_log_likelihood <- function(w_vector, sigma, X) { # built for no intercept models, as asked
  
  n <- nrow(X)
  term1 <- -n/2*log(2*pi*sigma^2)
  term2 <- -sum(sapply(1:n, function(x) X[x, 1] - X[x, -1] %*% w_vector)^2)/2/sigma^2 # assumes y = X[, 1]
  
  return(term1 + term2)
  
}

ridge <- function(w_vector_sigma, X, lambda) { # built for no intercept models, as asked
  
  n <- ncol(X)
  sigma <- w_vector_sigma[n]
  w_vector <- w_vector_sigma[1:(n-1)]
  return(lambda*sum(w_vector^2) - model_log_likelihood(w_vector, sigma, X))
  
}

ridge_opt <- function(X, lambda) {
  
  n <- ncol(X)
  par <- rnorm(n-1) # initial parameter values (to seed optimisation); random
  par[n] <- 1 # initialise with positive value
  return(optim(par = par, fn = ridge, method = "BFGS", X = X, lambda = lambda)$par)
  
}

degrees_of_freedom <- function(X, lambda) {
  
  mat <- tcrossprod(X %*% solve(crossprod(X) + diag(lambda, ncol(X), ncol(X))), X)
                    
  return(sum(diag(mat)))
                    
}

MSE <- function(X, optimal_weights) { # based on definition here: https://en.wikipedia.org/wiki/Mean_squared_error
  
  n <- nrow(X)
  return(sum((X[, 1] - X[, -1] %*% optimal_weights)^2)/n) # built for no-intercept models as asked
  
}

# model 1: lambda = 1
model1_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1)
model1_optimal_weights <- model1_optimal_weights_sigma[1:(n_col - 1)]
model1_optimal_sigma <- model1_optimal_weights_sigma[n_col]
model1_train_MSE <- MSE(train_data, model1_optimal_weights)
model1_test_MSE <- MSE(test_data, model1_optimal_weights)
model1_AIC <- -2*model_log_likelihood(
  model1_optimal_weights, model1_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), lambda = 1)

# model 2: lambda = 100
model2_optimal_weights_sigma <- ridge_opt(train_data, lambda = 100)
model2_optimal_weights <- model2_optimal_weights_sigma[1:(n_col - 1)]
model2_optimal_sigma <- model2_optimal_weights_sigma[n_col]
model2_train_MSE <- MSE(train_data, model2_optimal_weights)
model2_test_MSE <- MSE(test_data, model2_optimal_weights)
model2_AIC <- -2*model_log_likelihood(
  model2_optimal_weights, model2_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), lambda = 100)

# model 3: lambda = 1000
model3_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1000)
model3_optimal_weights <- model3_optimal_weights_sigma[1:(n_col - 1)]
model3_optimal_sigma <- model3_optimal_weights_sigma[n_col]
model3_train_MSE <- MSE(train_data, model3_optimal_weights)
model3_test_MSE <- MSE(test_data, model3_optimal_weights)
model3_AIC <- -2*model_log_likelihood(
  model3_optimal_weights, model3_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), lambda = 1000)

# plot performance
MSE_data <- data.frame(log_10_lambda = c(0, 0, 2, 2, 3, 3), Legend = rep(c("Training", "Test"), 3), 
                       value = c(model1_train_MSE, model1_test_MSE, model2_train_MSE, 
                                 model2_test_MSE, model3_train_MSE, model3_test_MSE))

ggplot(MSE_data) + geom_line(aes(log_10_lambda, value, colour = Legend)) +
  theme_bw() + geom_label(aes(log_10_lambda, value, label = round(value, 4))) + 
  geom_vline(xintercept = 2, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") + ylab("Mean Squared Error") + 
  ggtitle("Finding the Optimal Hyper-Parameter")
```

### 2.4.2 Most appropriate value of penalty parameter lambda

We can see that the optimal value of $\lambda$ within the set $\{1, 100, 1000\}$ is 100. At $\lambda = 1$, there is overfitting, evidenced by low training error and large test error; at $\lambda = 1000$, on the other hand, there is underfitting, evidenced by high training and test error.

### 2.4.3 MSE is a more appropriate measure here than other empirical risk functions

MSE is a more appropriate measure here than other empirical risk functions such as MAE (mean absolute error), because in regression problems, MSE can, unlike MAE and similar measures, also serve as an estimator of the variance of the error terms. Such an estimate would be useful in validating the underlying assumptions of a regression model (such as independence of error terms). Of course empirical risk functions such as mis-classification rate or cross-entropy do not apply in regression problems such as this, making MSE more appropriate than such measures.

### 2.5.1 Optimal model according to AIC criterion

In terms of AIC, the optimal value of $\lambda$ within the set $\{1, 100, 1000\}$ is 1 (see plot below); note that degrees of freedom are calculated according to Equation 3.50 in the book The Elements of Statistical Learning, Hastie et al, Second Edition, Springer, in page 68.

### 2.5.2 Theoretical advantage of AIC based model selection compared to holdout model selection

The theoretical advantage of using AIC for model selection is that it penalises models for high complexity (high degrees of freedom), which holdout model selection does not do; in reality, however, AIC is influenced both by data likelihood given model and degrees of freedom. In the plot below, we see evidence of AIC being dominated by the likelihood factor and in effect, we end up selecting the model $\lambda = 1$, as it has lowest AIC.

``` {r, echo = FALSE, fig.asp = 0.5}
AIC_data <- data.frame(log_10_lambda = c(0, 2, 3), value = c(model1_AIC, model2_AIC, model3_AIC))

ggplot(AIC_data) + geom_line(aes(log_10_lambda, value)) + theme_bw() + 
  geom_vline(xintercept = 0, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") + ylab("Model AIC") + 
  ggtitle("Finding the Optimal Hyper-Parameter")
```


# Part done by Tore so far

So 3.6 and 3.7, are not finished need to do the hypothisis test, and was abit unsure as what to plot thought it to be abit unclear for 3.6. 


### 3.1
```{r, echo=FALSE}
fats <- read.csv("tecator.csv", row.names = 1)
fats <- fats[, -c(102,103)]
set.seed(12345)
n3 <- nrow(fats)
id_fat_train <- sample(1:n3, floor(n3*0.5))
id_fat_test <- setdiff(1:n3, id_fat_train)

train_fat <- fats[id_fat_train,]
test_fat <- fats[id_fat_test,]
```



```{r,echo =FALSE}


# train_fat<-scale(train_fat)
# test_fat <- scale(test_fat)
train_fat <- as.data.frame(train_fat)
test_fat<-as.data.frame(test_fat)
lm_fats_train <- lm(Fat ~., data = train_fat )

#train_mse <- mean((train_fat - predict(lm_fats_train, train_fat))^2)

summary(lm_fats_train)

#predictions 
predicted_train_fat <- predict(lm_fats_train)

#train mse
mse_train <- mean((train_fat$Fat - predicted_train_fat)^2)

predicted_test_fat<-predict(lm_fats_train, test_fat)

#test mse
mse_test <- mean((test_fat$Fat- predicted_test_fat)^2)

cat("Training MSE: ", mse_train, ". Test MSE: ", mse_test)

```



With the very low training MSE and comparably high test MSE we have reason to believe that there is  overfitting occurring with the model. From viewing the summary for the training model it was found to give a adjusted R-square of 0.9994% which gives an clear indication that the model is overfit.

```{r, echo=FALSE}
par(mfrow = c(2,2))

plot(lm_fats_train)

```



Analyzing the plots we find that the assumptions that the model residuals are i.i.d can be assumed to be approximatly fullfilled, it's clear that the residuals do not follow anything similar to a normal distribution with mean 0 and variance 1 when observing the QQ-plot. Hence the assumptions for the model are not fulfilled so we should not draw any conclusions from the model.



### 3.2

The function we are looking to optimize in this case is loss function of the LASSO model.


$$\sum\limits_{i=1}^N{(y_i - \mathbb{X}_i\boldsymbol{\beta})^2} + \lambda\sum\limits_{j=1}^p{|\beta_j|}$$
Where $N$ is the number of observations and $\mathbb{X}$ is the matrix of features and y is the dependent variable. $\beta$ are the parameters that the shrinkage function right part of the plus sign. With $\lambda$ being the shrinkage parameter and P the number of features. Where the chosen $\lambda$ will shrink the $\beta$ paramters. 

### 3.3
```{r, echo=FALSE}

train_scale<-scale(train_fat)
test_scale <- scale(test_fat)

x<-as.matrix(train_scale[,c(1:100)])
y <- as.matrix(train_scale[,101])


lambda_values<-seq(from = 0.01, to = 0.1, by = 0.002)
lambda_values<-rev(lambda_values)
lasso3_3_df <- 0
i <- 1

#Finding where we get 3 parameters numerically
while(lasso3_3_df < 3){

lasso3_3 <- glmnet(x = x, y = y, alpha = 1, lambda = lambda_values[i])

lasso3_3_df <-   lasso3_3$df
i = i+1
}

log(lambda_values[i])

lasso3_3_for_plot<-glmnet(x = x, y = y, alpha = 1)

plot(lasso3_3_for_plot, "lambda" , main = "LASSO model coefficent convergence")


```

From the plot we can see that where the function converges to 3 features that have non zero coefficients is approximately at $e^[-2.6]$ and it was numerically shown that at $log(\lambda) = -2.688248$ the function would have a df of 3 and 3 features with non zero coefficents.

### 3.4

```{r, echo=FALSE}

plot_data <- data.frame("log_lambda" = log(lasso3_3_for_plot$lambda), "df" = lasso3_3_for_plot$df )
ggplot(data = plot_data ,aes( x= log_lambda , y = df)) + geom_point() + 
  labs(title = "Degrees of freedom vs Log lambda", x = "Log lambda", y = "Degrees of freedom")

```

The graph shows that as the value of the shrinkage parameter lambda increases it reduces the number of features in the model. Which is exactly as expected from the given function with an sufficiently large $\lambda$ it will reduce the number of features included to 0 as such decreasing the degrees of freedom. 

```{r, echo=FALSE}

lasso3_5 <- glmnet(x = x, y = y, alpha = 0)
plot(lasso3_5, "lambda")
log(lambda_values[i])

lasso3_5_for_plot<-glmnet(x = x, y = y, alpha = 0)

plot(lasso3_3_for_plot, "lambda", main = "Lasso regression: Coef vs log lambda")
plot(lasso3_5_for_plot, "lambda", main = "Ridge regression: Coef vs log Lambda")
```

As seen in the graphs the lasso function will converge the coefficients of the model to 0 whereas the ridge regression model will reduce the coefficients close to zero but never to zero.

```{r, echo=FALSE}
# cross validation with 10 folds
cv_lasso <- cv.glmnet(x = x, y = y, alpha = 1)
cv_lasso$lambda.min
plot(cv_lasso)

```

The graph shows that the lowest MSE is found at $\lambda = 0.004561105$ which is at $log(\lambda) = -5.39019$ as shown in the graph. As the $log(\lambda)$ increases the MSE will increase with a linear rate from -8 to approx -5 and then with a non-linear increase until approximately -2.8 and then where it will slowdown the increasing rate of MSE.

```{r, echo=FALSE}


lasso3_6_opt <- glmnet(x = x, y = y, alpha = 1, lambda = cv_lasso$lambda.min)

lasso3_6_opt$df
#We find that 7 variables were used in the 

lambda_test<-exp(-2)
lasso3_6_lambda_chosen <- glmnet(x = x, y = y, alpha = 1, lambda = lambda_test)

lasso3_6_lambda_chosen$df

#need to check if significantly diffrent

```

```{r}



```




```{r, echo = FALSE}

#plotting predicted vs observed values

y_hat_lasso <-predict(lasso3_6_opt, as.matrix(test_scale[,-101]))

plot_data_36 <- data.frame("ypredlm" = scale(predicted_test_fat) , "ylassopred" = y_hat_lasso)
ggplot(plot_data_36, aes(x = ypredlm, y = s0)) + geom_point() + labs(title = "Comparison of Predicted values with LASSO optimal vs \n predicted values from linear regression", x = "Predicted values from LM", y = "Predicted values from LASSO")
```




### 3.7

```{r}


preds <- predict(lasso3_6_opt, test_scale[,-101])

sigma<- sqrt(mean((train_scale[,101] - predict(lasso3_6_opt,train_scale[,-101]))^2 ))
new_targets <- rnorm(n = nrow(test_scale),preds, sigma )

generated_data <- data.frame("ytest" = test_scale[,101], "generated" = new_targets )
ggplot(generated_data, aes(x = ytest, y = generated)) + geom_point()


```




















## Appendix: Code

``` {r, eval = FALSE}
library(readr)
library(kknn)
library(ggplot2)
library(data.table)
library(glmnet)

#Assignment 1
#1.1
digits <- read.csv("optdigits.csv", header = F)
colnames(digits)[ncol(digits)] <- "number"
digits$number <- as.factor(digits$number)
n <- dim(digits)[1] 
set.seed(12345)
id <- sample(1:n, floor(n*0.5)) 
train <- digits[id,]
id1 <- setdiff(1:n, id) 
set.seed(12345) 
id2 <- sample(id1, floor(n*0.25)) 
validation <- digits[id2,]
id3 <- setdiff(id1,id2) 
test <- digits[id3,]

#1.2
knn_train <- kknn(number ~., train, train, k = 30, kernel = "rectangular")
knn_test <- kknn(number ~., train, test, k = 30, kernel = "rectangular")
#Confusion matrix
pred_train<- fitted(knn_train)
confusion_train <- table(train$number, pred_train)
pred_test <- fitted(knn_test)
confusion_test <- table(test$number, pred_test)
print(confusion_train)
print(confusion_test)
print(diag(confusion_train)/rowSums(confusion_train))
print(diag(confusion_test)/rowSums(confusion_test))
#Misclassification errors
missclass <- function(X,X1) { 
  n = length(X)
  return(1-sum(diag(table(X,X1)))/n)
}
mismatch_rate_train <- missclass(train$number, pred_train) #0.04500262
mismatch_rate_test <- missclass(test$number, pred_test) #0.05329154
cat("The mismatch rate for train data is:", mismatch_rate_train, 
    "\nThe mismatch rate for test data is:", mismatch_rate_test)

#1.3
train_eight <- train[which(train$number==8),]
train_eight$prob <- knn_train$prob[which(train$number==8), "8"]
sortedResult <- sort(train_eight[, "prob"], index.return = T)
sortedResult$x[1:3] #0.1000000 0.1333333 0.1666667
hard_ids <- sortedResult$ix[1:3] #50  43 136
tail(sortedResult$x, n=c(2)) #1 1
easy_ids <- tail(sortedResult$ix, n=c(2)) #179 183
plotHeatmap <- function(index){
  heatmap(matrix(as.numeric(train_eight[index, 1:64]), 8, 8, byrow = T), Colv = NA, Rowv = NA)
}
for (i in c(hard_ids, easy_ids)) {
  plotHeatmap(i)
}

#1.4
e_t <- 0
e_v <- 0
for (k in 1:30) {
  kt <- kknn(number ~., train, train, k = k, kernel = "rectangular")
  e_t[k] <- missclass(train$number, fitted(kt))
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  e_v[k] <- missclass(validation$number, fitted(kv))
}
plot(1:30, e_v, ylim = c(0,0.06), type = "l", col = "red", xlab = "K", ylab = "miss-classification error rate")
lines(1:30, e_t, ylim = c(0,0.06), col = "blue")
legend(2, 0.06, legend=c("validation", "train"),
       col=c("red", "blue"), lty=1:1, cex=0.8)

k_test_optik <- kknn(number ~., train, test, k = 4, kernel = "rectangular")
cat("Test mis-classification rate when k=4 is:", missclass(test$number, fitted(k_test_optik)),
    "\nTrain mis-classification rate when k=4 is:", e_t[4],
    "\nValidation mis-classification rate when k=4 is:", e_v[4])

#1.5
r_v <- 0
for (k in 1:30) {
  kv <- kknn(number ~., train, validation, k = k, kernel = "rectangular")
  s <- 0
  for (i in 0:9) {
    s <- s + sum(log(kv$prob[which(validation$number==i), toString(i)] + 1e-15)/nrow(kv$prob))
  }
  r_v[k] <- -s
}
plot(1:30, r_v, type = "l", xlab = "K", ylab = "mean cross entropy")


### Assignment 2

# read in data
parkinsons <- read_csv("parkinsons.csv", col_types = cols())

# drop columns not to be used for prediction
parkinsons <- parkinsons[, -c(1, 2, 3, 4, 6)]

# bayesian model

## $$\mathbf{y} | \mathbb{X}, w_0, \mathbf{w} \sim N(w_0 +
## \mathbb{X}\mathbf{w}, \sigma^2\cdot \mathbb{I})$$
## $$\mathbf{w} \sim N(0, \sigma^2/\lambda \cdot \mathbb{I})$$

# create train/test partitions
all_indices <- 1:(nrow(parkinsons))
set.seed(12345)
train_indices <- sample(all_indices, ceiling(0.6*nrow(parkinsons)))
test_indices <- all_indices[!(all_indices %in% train_indices)]
train_data <- parkinsons[train_indices, ]
test_data <- parkinsons[test_indices, ]

# scale data -- see https://sebastianraschka.com/faq/docs/scale-training-test.html
# for why training mu/sigma are used for scaling test data
n_col <- ncol(parkinsons)
train_mu <- sapply(1:n_col, function(x) mean(unlist(train_data[x])))
train_sigma <- sapply(1:n_col, function(x) sd(unlist(train_data[x])))

train_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (train_data[x] - train_mu[x])/train_sigma[x])))
# as.matrix has no effect without as.data.frame!!!
test_data <- as.matrix(as.data.frame(
  sapply(1:n_col, function(x) (test_data[x] - train_mu[x])/train_sigma[x])))

# set up helper functions

model_log_likelihood <- function(w_vector, sigma, X) { 
  # built for no intercept models, as asked
  
  n <- nrow(X)
  term1 <- -n/2*log(2*pi*sigma^2)
  term2 <- -sum(sapply(1:n, function(x) X[x, 1] - X[x, -1] %*% w_vector)^2)/2/sigma^2
  # assumes y = X[, 1]
  
  return(term1 + term2)
  
}

ridge <- function(w_vector_sigma, X, lambda) { 
  # built for no intercept models, as asked
  
  n <- ncol(X)
  sigma <- w_vector_sigma[n]
  w_vector <- w_vector_sigma[1:(n-1)]
  return(lambda*sum(w_vector^2) - model_log_likelihood(w_vector, sigma, X))
  
}

ridge_opt <- function(X, lambda) {
  
  n <- ncol(X)
  par <- rnorm(n-1) # initial parameter values (to seed optimisation); random
  par[n] <- 1 # initialise with positive value
  return(optim(par = par, fn = ridge, method = "BFGS", X = X, lambda = lambda)$par)
  
}

degrees_of_freedom <- function(X, lambda) {
  
  mat <- tcrossprod(X %*% solve(crossprod(X) + diag(lambda, ncol(X), ncol(X))), X)
                    
  return(sum(diag(mat)))
                    
}

MSE <- function(X, optimal_weights) { # based on definition here:
  # https://en.wikipedia.org/wiki/Mean_squared_error
  
  n <- nrow(X)
  return(sum((X[, 1] - X[, -1] %*% optimal_weights)^2)/n) 
  # built for no-intercept models as asked
  
}

# model 1: lambda = 1
model1_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1)
model1_optimal_weights <- model1_optimal_weights_sigma[1:(n_col - 1)]
model1_optimal_sigma <- model1_optimal_weights_sigma[n_col]
model1_train_MSE <- MSE(train_data, model1_optimal_weights)
model1_test_MSE <- MSE(test_data, model1_optimal_weights)
model1_AIC <- -2*model_log_likelihood(
  model1_optimal_weights, model1_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), lambda = 1)

# model 2: lambda = 100
model2_optimal_weights_sigma <- ridge_opt(train_data, lambda = 100)
model2_optimal_weights <- model2_optimal_weights_sigma[1:(n_col - 1)]
model2_optimal_sigma <- model2_optimal_weights_sigma[n_col]
model2_train_MSE <- MSE(train_data, model2_optimal_weights)
model2_test_MSE <- MSE(test_data, model2_optimal_weights)
model2_AIC <- -2*model_log_likelihood(
  model2_optimal_weights, model2_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), lambda = 100)

# model 3: lambda = 1000
model3_optimal_weights_sigma <- ridge_opt(train_data, lambda = 1000)
model3_optimal_weights <- model3_optimal_weights_sigma[1:(n_col - 1)]
model3_optimal_sigma <- model3_optimal_weights_sigma[n_col]
model3_train_MSE <- MSE(train_data, model3_optimal_weights)
model3_test_MSE <- MSE(test_data, model3_optimal_weights)
model3_AIC <- -2*model_log_likelihood(
  model3_optimal_weights, model3_optimal_sigma, as.matrix(train_data)) + 
  2*degrees_of_freedom(as.matrix(train_data), lambda = 1000)

# plot performance
MSE_data <- data.frame(log_10_lambda = c(0, 0, 2, 2, 3, 3), 
                       Legend = rep(c("Training", "Test"), 3), 
                       value = c(model1_train_MSE, model1_test_MSE, model2_train_MSE, 
                                 model2_test_MSE, model3_train_MSE, model3_test_MSE))

ggplot(MSE_data) + geom_line(aes(log_10_lambda, value, colour = Legend)) + theme_bw() + 
  geom_vline(xintercept = 2, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") + ylab("Mean Squared Error") + 
  ggtitle("Finding the Optimal Hyper-Parameter")

AIC_data <- data.frame(log_10_lambda = c(0, 2, 3), 
                       value = c(model1_AIC, model2_AIC, model3_AIC))

ggplot(AIC_data) + geom_line(aes(log_10_lambda, value)) + theme_bw() + 
  geom_vline(xintercept = 2, linetype = "dotted") + 
  xlab("Log (base-10) of Hyper-Parameter 'lambda'") + ylab("Model AIC") + 
  ggtitle("Finding the Optimal Hyper-Parameter")
```